{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOD/UiklVf1m7xtsl2iT+Mq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KelvinM9187/Supervised-Speech-Recognition-with-Transformers/blob/main/speech_recognition_twi_lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Install required dependencies\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q datasets transformers accelerate bitsandbytes peft evaluate jiwer soundfile torchaudio librosa\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "lCNyrq34nD1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from pathlib import Path\n",
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "from datasets import load_dataset, Dataset, Audio, DatasetDict\n",
        "from transformers import (\n",
        "    WhisperProcessor,\n",
        "    WhisperForConditionalGeneration,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "\n",
        "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "PREFIX_CHECKPOINT_DIR = \"checkpoint\"\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, PeftModel, prepare_model_for_kbit_training"
      ],
      "metadata": {
        "id": "IZr8Nreinbvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"openai/whisper-base\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/whisper_twi_checkpoints\"\n",
        "TARGET_HOURS = 4.0\n",
        "MAX_SAMPLES = 8000\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "# Data split ratios\n",
        "TRAIN_RATIO = 0.8\n",
        "VAL_RATIO = 0.1\n",
        "TEST_RATIO = 0.1\n",
        "\n",
        "# Training hyperparams\n",
        "PER_DEVICE_BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION_STEPS = 8\n",
        "LEARNING_RATE = 3e-4\n",
        "NUM_TRAIN_EPOCHS = 3\n",
        "SAVE_STEPS = 200\n",
        "EVAL_STEPS = 200\n",
        "LOGGING_STEPS = 50\n",
        "FP16 = True\n",
        "\n",
        "# LoRA config\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# reproducibility\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "id": "yzZDbj91scsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Twi dataset\n",
        "\n",
        "print(\"Loading Twi dataset from Common Voice...\")\n",
        "\n",
        "try:\n",
        "    # Try loading Common Voice with Twi language\n",
        "    dataset = load_dataset(\"mozilla-foundation/common_voice_16_1\", \"tw\")\n",
        "    print(\"Successfully loaded Common Voice Twi dataset!\")\n",
        "\n",
        "    # Get the splits\n",
        "    train_data = dataset['train']\n",
        "    val_data = dataset['validation']\n",
        "    test_data = dataset['test']\n",
        "\n",
        "    print(f\"Dataset sizes - Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
        "\n",
        "    # Create data dictionary\n",
        "    data_dict = DatasetDict({\n",
        "        \"train\": train_data,\n",
        "        \"validation\": val_data,\n",
        "        \"test\": test_data\n",
        "    })\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not load Common Voice Twi: {e}\")\n",
        "    print(\"Trying alternative approach with FLEURS dataset...\")\n",
        "\n",
        "    try:\n",
        "        # Try FLEURS dataset which contains Twi\n",
        "        dataset = load_dataset(\"google/fleurs\", \"tw_gh\")\n",
        "        print(\"Successfully loaded FLEURS Twi dataset!\")\n",
        "\n",
        "        train_data = dataset['train']\n",
        "        val_data = dataset['validation']\n",
        "        test_data = dataset['test']\n",
        "\n",
        "\n",
        "        data_dict = DatasetDict({\n",
        "            \"train\": train_data,\n",
        "            \"validation\": val_data,\n",
        "            \"test\": test_data\n",
        "        })\n",
        "\n",
        "        print(f\"Loaded FLEURS Twi: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test\")\n",
        "\n",
        "\n",
        "    except Exception as e2:\n",
        "        print(f\"Could not load FLEURS Twi: {e2}\")\n",
        "        print(\"Using LibriSpeech as fallback for testing...\")\n",
        "\n",
        "        # Fallback to LibriSpeech\n",
        "        dataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"train+validation+test\")\n",
        "\n",
        "        # Split manually\n",
        "        n = len(dataset)\n",
        "        n_train = int(n * TRAIN_RATIO)\n",
        "        n_val = int(n * VAL_RATIO)\n",
        "\n",
        "        train_data = dataset.select(range(0, n_train))\n",
        "        val_data = dataset.select(range(n_train, n_train + n_val))\n",
        "        test_data = dataset.select(range(n_train + n_val, n))\n",
        "\n",
        "        data_dict = DatasetDict({\n",
        "            \"train\": train_data,\n",
        "            \"validation\": val_data,\n",
        "            \"test\": test_data\n",
        "        })\n",
        "\n",
        "        print(f\"Using LibriSpeech fallback: {len(train_data)} train, {len(val_data)} val, {len(test_data)} test\")\n",
        "\n",
        "# Cast audio to fixed sampling rate for Whisper\n",
        "print(\"Converting audio to 16kHz...\")\n",
        "data_dict = data_dict.cast_column(\"audio\", Audio(sampling_rate=SAMPLE_RATE))"
      ],
      "metadata": {
        "id": "3s_wr7vQssoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model & processor\n",
        "print(\"Loading processor and model...\")\n",
        "processor = WhisperProcessor.from_pretrained(MODEL_NAME, task=\"transcribe\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ],
      "metadata": {
        "id": "rlRwNlGlzF0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess function\n",
        "MAX_DURATION_IN_SECONDS = 30.0\n",
        "MAX_INPUT_LENGTH = int(MAX_DURATION_IN_SECONDS * SAMPLE_RATE)\n",
        "MAX_LABEL_LENGTH = 448\n",
        "\n",
        "def prepare_example(batch):\n",
        "    try:\n",
        "        # audio is dataset.Audio with 'array' and 'sampling_rate'\n",
        "        audio = batch[\"audio\"]\n",
        "        array = audio[\"array\"]\n",
        "        sr = audio[\"sampling_rate\"]\n",
        "\n",
        "        # Handle cases where audio might be None\n",
        "        if array is None:\n",
        "            # Create silent audio as fallback\n",
        "            array = np.zeros(int(SAMPLE_RATE * 1.0))  # 1 second of silence\n",
        "            sr = SAMPLE_RATE\n",
        "\n",
        "        # Ensure audio is mono and correct length\n",
        "        if len(array.shape) > 1:\n",
        "            array = array.mean(axis=0)  # Convert to mono\n",
        "\n",
        "        # feature extractor\n",
        "        inputs = processor.feature_extractor(array, sampling_rate=sr)\n",
        "        # extract first (single) example features\n",
        "        batch[\"input_features\"] = inputs.input_features[0]\n",
        "\n",
        "        # Get transcription text\n",
        "        transcription = batch.get(\"sentence\") or batch.get(\"transcription\") or batch.get(\"text\") or \"\"\n",
        "\n",
        "        # labels\n",
        "        batch[\"labels\"] = processor.tokenizer(transcription).input_ids\n",
        "\n",
        "        # lengths for filtering\n",
        "        batch[\"input_length\"] = len(array)\n",
        "        batch[\"labels_length\"] = len(processor.tokenizer(transcription, add_special_tokens=False).input_ids)\n",
        "        return batch\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing example: {e}\")\n",
        "        # Return None to be filtered out\n",
        "        return None\n",
        "\n",
        "# Apply mapping with error handling\n",
        "print(\"Preparing dataset features...\")\n",
        "\n",
        "def safe_prepare_example(example):\n",
        "    try:\n",
        "        return prepare_example(example)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping example due to error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Filter out None results\n",
        "for split in [\"train\", \"validation\", \"test\"]:\n",
        "    print(f\"Processing {split} split...\")\n",
        "    current_columns = data_dict[split].column_names\n",
        "    data_dict[split] = data_dict[split].map(\n",
        "        safe_prepare_example,\n",
        "        remove_columns=current_columns,\n",
        "        num_proc=2\n",
        "    )\n",
        "    # Remove None results\n",
        "    original_len = len(data_dict[split])\n",
        "    data_dict[split] = data_dict[split].filter(lambda x: x is not None)\n",
        "    new_len = len(data_dict[split])\n",
        "    print(f\"{split}: {new_len}/{original_len} examples processed successfully\")\n",
        "\n",
        "# Filter by length\n",
        "def keep_example(example):\n",
        "    il = example.get(\"input_length\", 0)\n",
        "    ll = example.get(\"labels_length\", 0)\n",
        "    return (0 < il < MAX_INPUT_LENGTH) and (ll < MAX_LABEL_LENGTH)\n",
        "\n",
        "for split in [\"train\", \"validation\", \"test\"]:\n",
        "    original_len = len(data_dict[split])\n",
        "    data_dict[split] = data_dict[split].filter(keep_example)\n",
        "    filtered_len = len(data_dict[split])\n",
        "    print(f\"{split}: {filtered_len}/{original_len} after length filtering\")\n",
        "\n",
        "# Remove temporary columns\n",
        "for split in [\"train\", \"validation\", \"test\"]:\n",
        "    data_dict[split] = data_dict[split].remove_columns([\"input_length\", \"labels_length\"])"
      ],
      "metadata": {
        "id": "5mFQcJ7Ty-XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator for Whisper seq2seq\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # pad input features (feature_extractor)\n",
        "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # pad labels\n",
        "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding token id's of the labels by -100 so it's ignored by the loss\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # drop BOS\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "nEjWQDxwyxHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup LoRA (PEFT) and prepare model for training\n",
        "print(\"Preparing model for k-bit training and applying LoRA (PEFT)...\")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "e5w35J8wyqNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments & Trainer\n",
        "output_dir = OUTPUT_DIR\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=5,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=FP16,\n",
        "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
        "    predict_with_generate=True,\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=False,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        ")\n",
        "\n",
        "# Callback to save only adapter weights\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "        return control\n",
        "\n",
        "# Metrics: WER\n",
        "import evaluate\n",
        "metric = evaluate.load(\"wer\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    wer = metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"wer\": wer}\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=data_dict[\"train\"],\n",
        "    eval_dataset=data_dict[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[SavePeftModelCallback()],\n",
        "    tokenizer=processor.feature_extractor\n",
        ")"
      ],
      "metadata": {
        "id": "8p16j8c7yhn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpoint for Resuming Training Sessions\n",
        "def find_latest_checkpoint(output_dir):\n",
        "    ckpts = [d for d in os.listdir(output_dir) if d.startswith(PREFIX_CHECKPOINT_DIR)]\n",
        "    if not ckpts:\n",
        "        return None\n",
        "    ckpts_sorted = sorted(ckpts, key=lambda x: int(x.split(\"-\")[-1]))\n",
        "    return os.path.join(output_dir, ckpts_sorted[-1])\n",
        "\n",
        "latest_ckpt = None\n",
        "if os.path.exists(output_dir):\n",
        "    latest_ckpt = find_latest_checkpoint(output_dir)\n",
        "if latest_ckpt:\n",
        "    print(\"Found checkpoint:\", latest_ckpt)\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting fresh training.\")"
      ],
      "metadata": {
        "id": "Uh-M-zv9yYRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Session\n",
        "print(\"Starting training...\")\n",
        "train_result = trainer.train(resume_from_checkpoint=latest_ckpt if latest_ckpt else None)\n",
        "\n",
        "# Save final PEFT adapter & processor\n",
        "print(\"Saving adapter and processor to:\", output_dir)\n",
        "model.save_pretrained(output_dir)\n",
        "processor.save_pretrained(output_dir)\n",
        "\n",
        "# Save training state & metrics\n",
        "trainer.save_state()\n",
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"Evaluating on test set...\")\n",
        "eval_res = trainer.evaluate(eval_dataset=data_dict[\"test\"], max_length=256, num_beams=1)\n",
        "print(\"Test evaluation:\", eval_res)\n",
        "trainer.log_metrics(\"eval_test\", eval_res)\n",
        "trainer.save_metrics(\"eval_test\", eval_res)\n",
        "\n",
        "# Plotting loss & WER history\n",
        "log_hist = trainer.state.log_history\n",
        "steps = []\n",
        "losses = []\n",
        "eval_steps = []\n",
        "eval_wers = []\n",
        "for entry in log_hist:\n",
        "    if \"loss\" in entry:\n",
        "        steps.append(entry.get(\"step\", None))\n",
        "        losses.append(entry[\"loss\"])\n",
        "    if \"eval_wer\" in entry:\n",
        "        eval_steps.append(entry.get(\"step\", None))\n",
        "        eval_wers.append(entry[\"eval_wer\"])\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(8,4))\n",
        "if len(steps) > 0:\n",
        "    plt.plot(steps, losses, marker=\"o\")\n",
        "    plt.title(\"Training loss\")\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No training loss logs to plot.\")\n",
        "\n",
        "# Plot eval WER curve\n",
        "plt.figure(figsize=(8,4))\n",
        "if len(eval_steps) > 0:\n",
        "    plt.plot(eval_steps, eval_wers, marker=\"o\")\n",
        "    plt.title(\"Validation WER (lower is better)\")\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"WER\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No eval WER logs to plot.\")\n",
        "\n",
        "# Minimal inference example\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"Loading final PEFT model for inference...\")\n",
        "peft_model = PeftModel.from_pretrained(model, OUTPUT_DIR)\n",
        "peft_model.to(device)\n",
        "inference_pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=peft_model,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    device=0 if device == \"cuda\" else -1\n",
        ")\n",
        "\n",
        "# Test inference\n",
        "if len(data_dict[\"test\"]) > 0:\n",
        "    example_audio = data_dict[\"test\"][0][\"audio\"][\"array\"]\n",
        "    # Get transcription from available fields\n",
        "    transcription = data_dict[\"test\"][0].get(\"sentence\") or data_dict[\"test\"][0].get(\"transcription\") or data_dict[\"test\"][0].get(\"text\") or \"No transcription available\"\n",
        "    print(\"Example ground truth:\", transcription)\n",
        "    print(\"Model transcription:\", inference_pipe(example_audio)[\"text\"])\n",
        "else:\n",
        "    print(\"No test samples available for inference demo\")\n",
        "\n",
        "print(\"All done. Checkpoints and final adapter saved to:\", OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "LFnZTFEZyIhn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}