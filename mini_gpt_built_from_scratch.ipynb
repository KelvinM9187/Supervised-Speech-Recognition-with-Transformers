{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUKo2Voobqe7sOJsNo2T2S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KelvinM9187/Supervised-Speech-Recognition-with-Transformers/blob/main/mini_gpt_built_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRrReT2neQmD"
      },
      "outputs": [],
      "source": [
        "# Setting Up The Environment\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CHECKPOINT_ROOT = '/content/drive/MyDrive/mini_gpt_checkpoints'\n",
        "import os\n",
        "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)\n",
        "\n",
        "print(\"Checkpoints will be saved to:\", CHECKPOINT_ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary Libraries\n",
        "\n",
        "import math, time, sys, os, random\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 1337\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "id": "tWR0y3y4egFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Tiny Shakespeare dataset\n",
        "DATA_PATH = 'tiny_shakespeare.txt'\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    !wget -q https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O {DATA_PATH}\n",
        "    print(\"Downloaded tiny_shakespeare to\", DATA_PATH)\n",
        "else:\n",
        "    print(\"Found existing dataset:\", DATA_PATH)\n",
        "\n",
        "# show head\n",
        "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "    raw = f.read()\n",
        "print(\"Dataset length (chars):\", len(raw))\n",
        "print(\"First 200 chars:\\n\", raw[:200].replace('\\n','\\\\n'))\n"
      ],
      "metadata": {
        "id": "7kbjKMTxf1Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding & train/val/test split (80/10/10)\n",
        "# Build char-level vocabulary\n",
        "chars = sorted(list(set(raw)))\n",
        "vocab_size = len(chars)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "\n",
        "# encode / decode helpers\n",
        "def encode(s): return [stoi[c] for c in s]\n",
        "def decode(l): return ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(raw), dtype=torch.long)\n",
        "\n",
        "n = len(data)\n",
        "n_train = int(0.8 * n)\n",
        "n_val   = int(0.1 * n)\n",
        "n_test  = n - n_train - n_val\n",
        "train_data = data[:n_train].to(device)\n",
        "val_data   = data[n_train:n_train+n_val].to(device)\n",
        "test_data  = data[n_train+n_val:].to(device)\n",
        "\n",
        "print(f\"Split: train {len(train_data)}, val {len(val_data)}, test {len(test_data)}\")\n"
      ],
      "metadata": {
        "id": "YB8Zp3G-gF0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loader utility\n",
        "# Function to get random mini-batches of contiguous characters\n",
        "def get_batch(split, block_size, batch_size):\n",
        "    if split == 'train':\n",
        "        d = train_data\n",
        "    elif split == 'val':\n",
        "        d = val_data\n",
        "    elif split == 'test':\n",
        "        d = test_data\n",
        "    else:\n",
        "        raise ValueError(split)\n",
        "    ix = torch.randint(0, len(d) - block_size, (batch_size,))\n",
        "    x = torch.stack([d[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "# quick test\n",
        "b_x, b_y = get_batch('train', block_size=64, batch_size=4)\n",
        "print(\"Batch shapes:\", b_x.shape, b_y.shape)\n"
      ],
      "metadata": {
        "id": "Jcj4XcgUgRoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Hyperparameters\n",
        "block_size = 256   # context length\n",
        "batch_size = 64\n",
        "n_layers = 4\n",
        "n_heads  = 4\n",
        "d_model  = 256     # embedding dimension\n",
        "d_ff     = 1024    # feed-forward hidden layer\n",
        "dropout  = 0.1\n",
        "learning_rate = 3e-4\n",
        "max_iters = 3000\n",
        "eval_interval = 200\n",
        "save_interval = 500\n",
        "grad_clip = 1.0\n",
        "\n",
        "print(\"Hyperparameters set.\")\n"
      ],
      "metadata": {
        "id": "VQzqRJEHgZmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Implementation (Decoder-only mini-GPT)\n",
        "# We implement a minimal decoder-only transformer with causal masking.\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, n_heads, attn_dropout=0.0, proj_dropout=0.0):\n",
        "        super().__init__()\n",
        "        assert dim % n_heads == 0\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = dim // n_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "        self.proj_dropout = nn.Dropout(proj_dropout)\n",
        "        # causal mask is created dynamically in forward\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.qkv(x)  # (B, T, 3*C)\n",
        "        qkv = qkv.reshape(B, T, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # each: (B, heads, T, head_dim)\n",
        "\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # (B, heads, T, T)\n",
        "\n",
        "        # causal mask: allow positions j <= i\n",
        "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)  # (1,1,T,T)\n",
        "        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "        attn_probs = self.attn_dropout(attn_probs)\n",
        "\n",
        "        out = torch.matmul(attn_probs, v)  # (B, heads, T, head_dim)\n",
        "        out = out.transpose(1,2).contiguous().view(B, T, C)\n",
        "        out = self.proj(out)\n",
        "        out = self.proj_dropout(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, n_heads, mlp_hidden_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(dim)\n",
        "        self.attn = CausalSelfAttention(dim, n_heads, attn_dropout=dropout, proj_dropout=dropout)\n",
        "        self.ln2 = nn.LayerNorm(dim)\n",
        "        self.mlp = FeedForward(dim, mlp_hidden_dim, dropout=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, block_size, n_layers, n_heads, d_model, d_ff, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Parameter(torch.randn(1, block_size, d_model) * 0.01)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            TransformerBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.block_size, \"Sequence length longer than block size\"\n",
        "        tok_emb = self.token_emb(idx)              # (B, T, d_model)\n",
        "        pos_emb = self.pos_emb[:, :T, :]          # (1, T, d_model)\n",
        "        x = self.drop(tok_emb + pos_emb)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)                     # (B, T, vocab)\n",
        "        if targets is None:\n",
        "            return logits\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "# instantiate model\n",
        "model = MiniGPT(vocab_size=vocab_size, block_size=block_size, n_layers=n_layers,\n",
        "                n_heads=n_heads, d_model=d_model, d_ff=d_ff, dropout=dropout).to(device)\n",
        "\n",
        "print(\"Model size (parameters):\", sum(p.numel() for p in model.parameters())/1e6, \"M\")\n"
      ],
      "metadata": {
        "id": "dL7fmJdignv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer, Scheduler & Utility Functions\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-1)\n",
        "# simple cosine lr schedule with warmup\n",
        "def get_scheduler(optimizer, warmup_steps=200, total_steps=max_iters):\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return float(step) / float(max(1, warmup_steps))\n",
        "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "scheduler = get_scheduler(optimizer)\n",
        "\n",
        "# checkpoint helpers\n",
        "def save_checkpoint(step, model, optimizer, scheduler, path):\n",
        "    state = {\n",
        "        'step': step,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optim_state_dict': optimizer.state_dict(),\n",
        "        'sched_state_dict': scheduler.state_dict()\n",
        "    }\n",
        "    torch.save(state, path)\n",
        "\n",
        "def load_checkpoint(path, model, optimizer=None, scheduler=None, map_location=device):\n",
        "    checkpoint = torch.load(path, map_location=map_location)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optim_state_dict'])\n",
        "    if scheduler is not None:\n",
        "        scheduler.load_state_dict(checkpoint.get('sched_state_dict', {}))\n",
        "    return checkpoint['step']\n",
        "\n",
        "# text generation (sampling)\n",
        "@torch.no_grad()\n",
        "def sample(model, start_text, length=500, temperature=1.0, top_k=None):\n",
        "    model.eval()\n",
        "    idx = torch.tensor(encode(start_text), dtype=torch.long, device=device).unsqueeze(0)  # (1, T)\n",
        "    for _ in range(length):\n",
        "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
        "        logits = model(idx_cond)[:, -1, :]  # (1, vocab)\n",
        "        logits = logits / (temperature if temperature > 0 else 1.0)\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, top_k)\n",
        "            minv = v[:, -1].unsqueeze(1)\n",
        "            logits = torch.where(logits < minv, torch.full_like(logits, -1e10), logits)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, next_id), dim=1)\n",
        "    return decode(idx.squeeze().tolist())\n",
        "\n",
        "print(\"Sampling function ready.\")\n"
      ],
      "metadata": {
        "id": "B8K_na5ug3qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "# Tracks losses and plots, saves checkpoints and samples.\n",
        "model.train()\n",
        "\n",
        "train_losses, val_losses, iters = [], [], []\n",
        "best_val_loss = 1e9\n",
        "start_iter = 0\n",
        "\n",
        "# checkpoint\n",
        "ckpt_path = '/content/drive/MyDrive/mini_gpt_checkpoints/ckpt_step_1000.pt'\n",
        "if ckpt_path and os.path.exists(ckpt_path):\n",
        "    start_iter = load_checkpoint(ckpt_path, model, optimizer, scheduler)\n",
        "    print(\"Resumed from\", ckpt_path, \"at iter\", start_iter)\n",
        "\n",
        "for it in range(start_iter + 1, max_iters + 1):\n",
        "    xb, yb = get_batch('train', block_size, batch_size)\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "    iters.append(it)\n",
        "\n",
        "    # periodic eval on validation set\n",
        "    if it % eval_interval == 0 or it == 1:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # compute val loss on a few batches to get a stable estimate\n",
        "            val_loss_vals = []\n",
        "            val_batches = 10\n",
        "            for _ in range(val_batches):\n",
        "                xb_val, yb_val = get_batch('val', block_size, batch_size)\n",
        "                _, vloss = model(xb_val, yb_val)\n",
        "                val_loss_vals.append(vloss.item())\n",
        "            mean_val_loss = sum(val_loss_vals) / len(val_loss_vals)\n",
        "            val_losses.append(mean_val_loss)\n",
        "        model.train()\n",
        "\n",
        "        # compute perplexity\n",
        "        train_pp = math.exp(loss.item())\n",
        "        val_pp = math.exp(mean_val_loss)\n",
        "        print(f\"Iter {it}/{max_iters} | train loss {loss.item():.4f} | val loss {mean_val_loss:.4f} | train pp {train_pp:.2f} | val pp {val_pp:.2f}\")\n",
        "\n",
        "        # generate a sample and save it\n",
        "        sample_text = sample(model, start_text=\"ROMEO: \", length=400, temperature=1.0, top_k=40)\n",
        "        sample_file = os.path.join(CHECKPOINT_ROOT, f\"sample_iter_{it}.txt\")\n",
        "        with open(sample_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(sample_text)\n",
        "        print(\"Sample saved to:\", sample_file)\n",
        "\n",
        "    # periodic checkpoint save\n",
        "    if it % save_interval == 0 or it == max_iters:\n",
        "        ckpt_file = os.path.join(CHECKPOINT_ROOT, f'ckpt_step_{it}.pt')\n",
        "        save_checkpoint(it, model, optimizer, scheduler, ckpt_file)\n",
        "        print(\"Saved checkpoint to\", ckpt_file)\n",
        "\n",
        "# After training, save final\n",
        "final_path = os.path.join(CHECKPOINT_ROOT, 'ckpt_final.pt')\n",
        "save_checkpoint(max_iters, model, optimizer, scheduler, final_path)\n",
        "print(\"Training complete. Final checkpoint saved to:\", final_path)\n"
      ],
      "metadata": {
        "id": "nVDXYWe_hF1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation loss and perplexity\n",
        "# Simple plotting\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(iters, train_losses, label='train_loss')\n",
        "\n",
        "# val_losses correspond to eval intervals\n",
        "eval_iters = [i for i in iters if i % eval_interval == 0 or i == 1]\n",
        "plt.plot(eval_iters[:len(val_losses)], val_losses, label='val_loss')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss curve')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(iters, [math.exp(x) for x in train_losses], label='train_perplexity')\n",
        "plt.plot(eval_iters[:len(val_losses)], [math.exp(x) for x in val_losses], label='val_perplexity')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Perplexity')\n",
        "plt.legend()\n",
        "plt.title('Perplexity')\n",
        "plt.grid(True)\n",
        "\n",
        "plot_path = os.path.join(CHECKPOINT_ROOT, 'training_plots.png')\n",
        "plt.savefig(plot_path)\n",
        "plt.show()\n",
        "print(\"Plots saved to:\", plot_path)\n"
      ],
      "metadata": {
        "id": "skAuToNChV-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final evaluation on test set\n",
        "# compute test loss on some batches\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_batches = 20\n",
        "    losses = []\n",
        "    for _ in range(test_batches):\n",
        "        xb_test, yb_test = get_batch('test', block_size, batch_size)\n",
        "        _, tloss = model(xb_test, yb_test)\n",
        "        losses.append(tloss.item())\n",
        "    test_loss = sum(losses) / len(losses)\n",
        "    test_pp = math.exp(test_loss)\n",
        "print(f\"Test loss: {test_loss:.4f} | Test perplexity: {test_pp:.2f}\")\n"
      ],
      "metadata": {
        "id": "VyL0CofNhiDN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}