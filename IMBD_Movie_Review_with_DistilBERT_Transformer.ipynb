{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KelvinM9187/Supervised-Speech-Recognition-with-Transformers/blob/main/IMBD_Movie_Review_with_DistilBERT_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install and import\n",
        "!pip install -q transformers datasets accelerate evaluate scikit-learn pandas matplotlib seaborn\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "import evaluate\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ],
      "metadata": {
        "id": "iLXEag_CEAWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTyFLrSmCy7a"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "class Config:\n",
        "    MODEL_NAME = \"distilbert-base-uncased\"\n",
        "    MAX_LENGTH = 512\n",
        "    BATCH_SIZE = 16\n",
        "    LEARNING_RATE = 2e-5\n",
        "    EPOCHS = 20\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    WARMUP_STEPS = 500\n",
        "    LOGGING_STEPS = 100\n",
        "    EVAL_STRATEGY = \"epoch\"\n",
        "    SAVE_STRATEGY = \"epoch\"\n",
        "    LOAD_BEST_MODEL_AT_END = True\n",
        "    METRIC_FOR_BEST_MODEL = \"eval_f1\"\n",
        "    GREATER_IS_BETTER = True\n",
        "\n",
        "    # Data\n",
        "    TRAIN_SAMPLES = 25000\n",
        "    TEST_SAMPLES = 25000\n",
        "    VAL_SPLIT = 0.1\n",
        "\n",
        "    # Output\n",
        "    OUTPUT_DIR = \"./imdb-sentiment-model\"\n",
        "    REPORT_PATH = \"./classification_report.txt\"\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Preparing IMDB dataset\n",
        "print(\"Loading IMDB dataset...\")\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Create validation split from training data\n",
        "train_testvalid = dataset['train'].train_test_split(\n",
        "    test_size=config.VAL_SPLIT,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Create DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'validation': train_testvalid['test'],\n",
        "    'test': dataset['test']\n",
        "})\n",
        "\n",
        "print(f\"Train: {len(dataset['train'])} samples\")\n",
        "print(f\"Validation: {len(dataset['validation'])} samples\")\n",
        "print(f\"Test: {len(dataset['test'])} samples\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nSample review:\")\n",
        "print(dataset['train'][0]['text'][:200])\n",
        "print(f\"Label: {'Positive' if dataset['train'][0]['label'] == 1 else 'Negative'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "print(f\"\\nLoading tokenizer for {config.MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=config.MAX_LENGTH\n",
        "    )\n",
        "\n",
        "print(\"Tokenizing datasets...\")\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# Metrics\n",
        "def compute_metrics(p):\n",
        "    preds, labels = p\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
        "    }\n"
      ],
      "metadata": {
        "id": "2KlACz0qD1jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Model\n",
        "print(f\"\\nLoading model {config.MODEL_NAME}...\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    config.MODEL_NAME,\n",
        "    num_labels=2,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model moved to: {device}\")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=config.OUTPUT_DIR,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=config.EPOCHS,\n",
        "    per_device_train_batch_size=config.BATCH_SIZE,\n",
        "    per_device_eval_batch_size=config.BATCH_SIZE * 2,\n",
        "    learning_rate=config.LEARNING_RATE,\n",
        "    weight_decay=config.WEIGHT_DECAY,\n",
        "    warmup_steps=config.WARMUP_STEPS,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=config.LOGGING_STEPS,\n",
        "    eval_strategy=config.EVAL_STRATEGY,\n",
        "    save_strategy=config.SAVE_STRATEGY,\n",
        "    load_best_model_at_end=config.LOAD_BEST_MODEL_AT_END,\n",
        "    metric_for_best_model=config.METRIC_FOR_BEST_MODEL,\n",
        "    greater_is_better=config.GREATER_IS_BETTER,\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_result.metrics)\n",
        "trainer.save_metrics(\"train\", train_result.metrics)\n",
        "trainer.save_state()\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "\n",
        "# Training and validation\n",
        "logs = trainer.state.log_history\n",
        "\n",
        "train_loss = []\n",
        "eval_loss = []\n",
        "eval_accuracy = []\n",
        "eval_f1 = []\n",
        "epochs = []\n",
        "\n",
        "for entry in logs:\n",
        "    if 'loss' in entry and 'epoch' in entry:\n",
        "        train_loss.append(entry['loss'])\n",
        "    if 'eval_loss' in entry:\n",
        "        eval_loss.append(entry['eval_loss'])\n",
        "        eval_accuracy.append(entry['eval_accuracy'])\n",
        "        eval_f1.append(entry['eval_f1'])\n",
        "        epochs.append(entry['epoch'])\n",
        "\n",
        "# Plots\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(train_loss, label=\"Training Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.xlabel(\"Training Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(epochs, eval_loss, marker='o', label=\"Validation Loss\")\n",
        "plt.title(\"Validation Loss per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(epochs, eval_accuracy, marker='o', label=\"Validation Accuracy\")\n",
        "plt.title(\"Validation Accuracy per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(epochs, eval_f1, marker='o', label=\"Validation F1 Score\")\n",
        "plt.title(\"Validation F1 Score per Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"F1 Score\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fDohIVh6DYWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test set evaluation\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EVALUATING ON TEST SET\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
        "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
        "true_labels = predictions.label_ids\n",
        "\n",
        "accuracy = accuracy_score(true_labels, pred_labels)\n",
        "f1 = f1_score(true_labels, pred_labels, average=\"weighted\")\n",
        "\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig('./confusion_matrix.png', dpi=100, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(true_labels, pred_labels,\n",
        "                               target_names=['Negative', 'Positive'],\n",
        "                               digits=4)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n",
        "\n",
        "with open(config.REPORT_PATH, 'w') as f:\n",
        "    f.write(f\"Model: {config.MODEL_NAME}\\n\")\n",
        "    f.write(f\"Test Accuracy: {accuracy:.4f}\\n\")\n",
        "    f.write(f\"Test F1-Score: {f1:.4f}\\n\\n\")\n",
        "    f.write(\"Classification Report:\\n\")\n",
        "    f.write(report)"
      ],
      "metadata": {
        "id": "1fpNHluHC-rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample predictions\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SAMPLE PREDICTIONS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "def predict_sentiment(text, model, tokenizer):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=config.MAX_LENGTH)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        pred_label = torch.argmax(probs, dim=1).item()\n",
        "\n",
        "    sentiment = \"Positive\" if pred_label == 1 else \"Negative\"\n",
        "    confidence = probs[0][pred_label].item()\n",
        "    return sentiment, confidence\n",
        "\n",
        "test_samples = [\n",
        "    \"This movie was absolutely fantastic! The acting was superb.\",\n",
        "    \"A complete waste of time. Poor acting and terrible script.\",\n",
        "    \"It was okay. Not great, but not terrible either.\",\n",
        "    \"One of the worst films I've ever seen.\",\n",
        "    \"Brilliant cinematography and outstanding performances!\"\n",
        "]\n",
        "\n",
        "for i, review in enumerate(test_samples):\n",
        "    sentiment, confidence = predict_sentiment(review, model, tokenizer)\n",
        "    print(f\"\\nReview {i+1}: {review[:80]}...\")\n",
        "    print(f\" â†’ Sentiment: {sentiment} (Confidence: {confidence:.2%})\")\n",
        "\n",
        "print(\"\\nSaving final model...\")\n",
        "model.save_pretrained(config.OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(config.OUTPUT_DIR)\n",
        "\n",
        "print(f\"\\nModel saved to: {config.OUTPUT_DIR}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINISHED!\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nGPU Memory allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
        "    print(f\"GPU Memory cached: {torch.cuda.memory_reserved(0)/1024**3:.2f} GB\")\n"
      ],
      "metadata": {
        "id": "KT_4-35zC5g_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}